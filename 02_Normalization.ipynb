{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing and Normalization\n",
    "\n",
    "Diphthongs and outlier productions will be excluded from analysis. Relative duration of vowels will be coded in addition to stress. Lastly, all formant data will be normalized following Delta F normalization (Johnson, 2018)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2006"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import df\n",
    "\n",
    "data = pd.read_csv(\"data/allvowels.csv\")\n",
    "print(len(data))\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of diphthongs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes df input with columns `next_ph` and `prev_ph` and returns df without diphthongs\n",
    "\n",
    "def rem_diph(df):\n",
    "    print(\"Initial length: \", len(df))\n",
    "    \n",
    "    df = df[(df['next_ph']!=\"a\") & \n",
    "                 (df['next_ph']!=\"e\") & \n",
    "                 (df['next_ph']!=\"i\") &\n",
    "                 (df['next_ph']!=\"o\") &\n",
    "                 (df['next_ph']!=\"u\")]\n",
    "    df = df[(df['prev_ph']!=\"a\") & \n",
    "                 (df['prev_ph']!=\"e\") & \n",
    "                 (df['prev_ph']!=\"i\") &\n",
    "                 (df['prev_ph']!=\"o\") &\n",
    "                 (df['prev_ph']!=\"u\")]\n",
    "    df = df.reset_index(drop = True)\n",
    "    \n",
    "    print(\"Final length: \", len(df))\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = rem_diph(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes productions with outlier formants\n",
    "# input df has columns `Participant` and formants listed in format `F1 (Hz)`\n",
    "\n",
    "def rem_outliers(df):\n",
    "    print(\"Initial length: \", len(df))\n",
    "    \n",
    "    # establish 25% and 75% for each formant\n",
    "    f1_qrts = df.groupby(['Participant', 'Vowel'])[\"F1 (Hz)\"].describe()[['25%', '75%']]\n",
    "    f2_qrts = df.groupby(['Participant', 'Vowel'])[\"F2 (Hz)\"].describe()[['25%', '75%']]\n",
    "    f3_qrts = df.groupby(['Participant', 'Vowel'])[\"F3 (Hz)\"].describe()[['25%', '75%']]\n",
    "    \n",
    "    # find interquartile range for each formant\n",
    "    f1_qrts['IQR'] = f1_qrts['75%'] - f1_qrts['25%']\n",
    "    f2_qrts['IQR'] = f2_qrts['75%'] - f2_qrts['25%']\n",
    "    f3_qrts['IQR'] = f3_qrts['75%'] - f3_qrts['25%']\n",
    "    \n",
    "    # determine upper limit for each formant\n",
    "    f1_qrts['upper'] = f1_qrts['75%'] + (1.5 * f1_qrts['IQR'])\n",
    "    f2_qrts['upper'] = f2_qrts['75%'] + (1.5 * f2_qrts['IQR'])\n",
    "    f3_qrts['upper'] = f3_qrts['75%'] + (1.5 * f3_qrts['IQR'])\n",
    "    \n",
    "    # determine lower limit for each formant\n",
    "    f1_qrts['lower'] = f1_qrts['25%'] - (1.5 * f1_qrts['IQR'])\n",
    "    f2_qrts['lower'] = f2_qrts['25%'] - (1.5 * f2_qrts['IQR'])\n",
    "    f3_qrts['lower'] = f3_qrts['25%'] - (1.5 * f3_qrts['IQR'])\n",
    "    \n",
    "    # create smaller df with only limits for each formant\n",
    "    f1_limits = f1_qrts[['upper','lower']]\n",
    "    f2_limits = f2_qrts[['upper','lower']]\n",
    "    f3_limits = f3_qrts[['upper','lower']]\n",
    "    \n",
    "    # merge limits into original df\n",
    "    df = df.merge(f1_limits, left_on = [\"Participant\", \"Vowel\"], right_index = True)\n",
    "    df = df.merge(f2_limits, left_on = [\"Participant\", \"Vowel\"], right_index = True, suffixes = (\"_f1\", \"_f2\"))\n",
    "    df = df.merge(f3_limits, left_on = [\"Participant\", \"Vowel\"], right_index = True)\n",
    "    \n",
    "    # drop rows with outlier formants\n",
    "    df = df[(df[\"F1 (Hz)\"] > df[\"lower_f1\"]) & (df[\"F1 (Hz)\"] < df[\"upper_f1\"])]\n",
    "    df = df[(df[\"F2 (Hz)\"] > df[\"lower_f2\"]) & (df[\"F2 (Hz)\"] < df[\"upper_f2\"])]\n",
    "    df = df[(df[\"F3 (Hz)\"] > df[\"lower\"]) & (df[\"F3 (Hz)\"] < df[\"upper\"])]\n",
    "\n",
    "    print(\"Final length: \", len(df))\n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = rem_outliers(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech rate\n",
    "\n",
    "First we will take the number of vowels a speaker produces to be equal to the number of syllables they utter. Then we will take the unique values from the `t1_wd` and `t2_wd` columns and subtract t2 from t1 to obtain an array of the duration of each word uttered. Then we will sum the durations of all words and divide the number of syllables by this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_rate(df):\n",
    "    import numpy as np\n",
    "\n",
    "    Participant = []\n",
    "    speech_rate = []\n",
    "\n",
    "    for i in df.Participant.unique():\n",
    "        data = vowels[vowels[\"Participant\"]==i]\n",
    "        syllables = len(data.Vowel)\n",
    "        end_times = data.t2_wd.unique()\n",
    "        start_times = data.t1_wd.unique()\n",
    "        durations = np.subtract(end_times, start_times)\n",
    "        duration = sum(durations)\n",
    "        rate = syllables/duration\n",
    "    \n",
    "        Participant.append(i)\n",
    "        speech_rate.append(rate)\n",
    "\n",
    "    rates = {k:v for k,v in zip(Participant, speech_rate)}\n",
    "    rates_df = pd.DataFrame.from_dict(rates, orient = \"index\", columns = ['Speech Rate'])\n",
    "    rates_df = rates_df.rename_axis('Participant').reset_index()\n",
    "    \n",
    "    df = pd.merge(left = df, right = rates_df, on = 'Participant', how = 'outer')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = speech_rate(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stress\n",
    "\n",
    "The syltippy package (https://github.com/nur-ag/syltippy) will be used to generate syllabified (stress-indicated) outputs for each word found in the transcriptions. Then, the corresponding vowels in the TextGrid-formant dataframes will be marked as either stressed or unstressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function takes into dictionary.txt file with cols `word` and `ipa`\n",
    "# input formants df with cols `Participant`, `word`, `t1_wd`, and `t1_ph`\n",
    "\n",
    "def get_stress(df):\n",
    "    # import required packages\n",
    "    import pandas as pd\n",
    "    import csv\n",
    "    import re\n",
    "    import numpy as np\n",
    "    from syltippy import syllabize\n",
    "    \n",
    "    formants = df.copy()\n",
    "    \n",
    "    # def fxn to create stress column in dictionary\n",
    "    def stress(word):\n",
    "        syllables, stress = syllabize(word)\n",
    "        return ','.join(s if stress != i else s.upper() for (i, s) in enumerate(syllables))\n",
    "    \n",
    "    # add column to dictionary\n",
    "    formants[\"stress_pattern\"] = formants[\"word\"].apply(lambda x : stress(x))\n",
    "    \n",
    "    # create separate column to hold only the vowels in each word\n",
    "    formants[\"stress_vowels\"] = formants[\"stress\"].apply(lambda x: re.sub(r'[^,aeiouAEIOUáéíóúÁÉÍÓÚ]', '', x))\n",
    "    \n",
    "    # define function to return index of 'vowels' column with stress\n",
    "    def is_stress(word):\n",
    "        # convert to list\n",
    "        word = word.split(\",\")\n",
    "        for syllable in word:\n",
    "            if syllable.isupper():\n",
    "                stress = word.index(syllable)\n",
    "        return stress\n",
    "    \n",
    "    # create new column which gives vowel number in given word that has stress\n",
    "    formants[\"stress_index\"] = formants[\"vowels\"].apply(lambda x: is_stress(x))\n",
    "    \n",
    "    # save as new variable\n",
    "    stress_indices = formants[['word', 'stress_index']].copy()\n",
    "    \n",
    "    # add column to formants indicating stressed vowel index\n",
    "    df = df.merge(stress_indices, on = \"word\", how = \"outer\")\n",
    "    df = df.sort_values([\"Participant\", \"t1_wd\", \"t1_ph\"])\n",
    "    \n",
    "    # add column to formants indicating index of vowel in each row\n",
    "    df[\"vowel_ind\"] = df.groupby([\"Participant\", \"t1_wd\"]).cumcount()\n",
    "    df = df.reset_index(drop = True)\n",
    "    \n",
    "    # add column to formants to indicate stress\n",
    "    df[\"stress\"] = np.where(df['stress_index'] == df['vowel_ind'], \"stressed\", \"unstressed\")\n",
    "       \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_stress(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization of vowel formants\n",
    "\n",
    "Because both male and female speakers are represented in this data set, the formant frequencies need to be normalized to minimized vocal tract length differences.\n",
    "\n",
    "Following Johnson (2018), I will use the line-fitting Delta F Normalization method, which makes use of the entire vowel space. To do so, the average vowel space will be calculated for each participant, and then each F1 and F2 measurement will be divided by this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def delta_f(vowels): # df as argument\n",
    "    \n",
    "    Participant = []\n",
    "    ll = []\n",
    "    \n",
    "    for i in vowels.Participant.unique():\n",
    "        data = vowels[vowels['Participant']==i]\n",
    "        \n",
    "        delta = np.mean([np.true_divide(data['F1 (Hz)'], 0.5), \n",
    "                        np.true_divide(data['F2 (Hz)'], 1.5), \n",
    "                        np.true_divide(data['F3 (Hz)'], 2.5)\n",
    "                       ])\n",
    "        \n",
    "        Participant.append(i)\n",
    "        ll.append(delta)\n",
    "    \n",
    "    deltas = {k:v for k,v in zip(Participant, ll)}\n",
    "    delta_df = pd.DataFrame.from_dict(deltas, orient = \"index\", columns = ['Delta F'])\n",
    "    delta_df = delta_df.rename_axis('Participant').reset_index()\n",
    "        \n",
    "    return(delta_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(vowels):\n",
    "    delta_df = delta_f(vowels)\n",
    "    \n",
    "    vowels_normalized = pd.merge(left = vowels,\n",
    "                                 right = delta_df,\n",
    "                                 on = 'Participant',\n",
    "                                 how = 'outer')\n",
    "    vowels_normalized['F1_norm'] = vowels_normalized['F1 (Hz)']/vowels_normalized['Delta F']\n",
    "    vowels_normalized['F2_norm'] = vowels_normalized['F2 (Hz)']/vowels_normalized['Delta F']\n",
    "    \n",
    "    return(vowels_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = normalization(data)\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"data/allnormdata.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
